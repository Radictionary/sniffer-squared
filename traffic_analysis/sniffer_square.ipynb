{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LongcnHhhJx",
        "outputId": "6df34c16-7cab-4952-b30e-a9133f8c27ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive/MyDrive/traffic/CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\n",
            "drive/MyDrive/traffic/CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv\n",
            "(1011957, 23)\n"
          ]
        }
      ],
      "source": [
        "def find_csv_delimiter(file_path, max_lines=5):\n",
        "    with open(file_path, 'r', newline='') as file:\n",
        "        sample_lines = [file.readline().strip() for _ in range(max_lines)]\n",
        "\n",
        "    delimiters = [',', ';', '\\t', '|']  # Common delimiters to check\n",
        "\n",
        "    best_delimiter = ','\n",
        "    max_delimiter_count = 0\n",
        "\n",
        "    for delimiter in delimiters:\n",
        "        delimiter_count = sum(line.count(delimiter) for line in sample_lines)\n",
        "        if delimiter_count > max_delimiter_count:\n",
        "            best_delimiter = delimiter\n",
        "            max_delimiter_count = delimiter_count\n",
        "\n",
        "    return best_delimiter\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def extract_data_from_csv(file_path, delimiter=','):\n",
        "    data = []  # Create a list to store the data\n",
        "\n",
        "    # Open the CSV file for reading\n",
        "    with open(file_path, mode='r', newline='') as file:\n",
        "        # Create a CSV reader object with the pipe delimiter\n",
        "\n",
        "        csv_reader = csv.reader(file, delimiter=delimiter)\n",
        "\n",
        "        # Read the header row\n",
        "        header = next(csv_reader)\n",
        "\n",
        "        # Iterate through the rows in the CSV file\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "\n",
        "    return data\n",
        "\n",
        "data = []\n",
        "\n",
        "import os\n",
        "cpt = 0\n",
        "for dirname, _, filenames in os.walk('drive/MyDrive/traffic'):\n",
        "\n",
        "        for filename in filenames:\n",
        "            if cpt < 2:\n",
        "                file_path = os.path.join(dirname, filename)\n",
        "                delimiter = find_csv_delimiter(file_path)\n",
        "                data += extract_data_from_csv(file_path, delimiter)\n",
        "                print(os.path.join(dirname, filename))\n",
        "                cpt+=1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "\n",
        "# Convert your data to a NumPy array\n",
        "data = np.array(data)\n",
        "\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYSWIjsi3QNM",
        "outputId": "9d40a1db-2f12-4ba6-9b0c-32f8532eede1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1525879831.015811' 'CUmrqr4svHuSXJy5z7' '192.168.100.103' '51524'\n",
            " '65.127.233.163' '23' 'tcp' '-' '2.999051' '0' '0' 'S0' '-' '-' '0' 'S'\n",
            " '3' '180' '0' '0' '-' 'Malicious' 'PartOfAHorizontalPortScan']\n"
          ]
        }
      ],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nV-0GwHS4cBU"
      },
      "outputs": [],
      "source": [
        "columns_to_remove = [0, 1, 2, 4, 12, 13, 14, 20, 22]\n",
        "\n",
        "# Remove columns using NumPy's array slicing\n",
        "data = np.delete(data, columns_to_remove, axis=1)\n",
        "\n",
        "# Make Malicious = 1 and Benign = 0\n",
        "for row in data:\n",
        "    if row[-1] == 'Benign':\n",
        "        row[-1] = 0\n",
        "    else:\n",
        "        row[-1] = 1\n",
        "\n",
        "columns_to_transform = [3, 4, 5, 6, 8]\n",
        "# Columns to put 0 if '-'\n",
        "zeros = [4, 5, 6]\n",
        "# Replace '-'\n",
        "for row in data:\n",
        "    for column in columns_to_transform:\n",
        "        if row[column] == '-' and column in zeros:\n",
        "            row[column] = 0\n",
        "        elif row[column] == '-' and column not in zeros:\n",
        "            row[column] = 'Unkown'\n",
        "\n",
        "columns_to_convert_to_float = [4]\n",
        "# Convert columns to float\n",
        "for row in data:\n",
        "    for column in columns_to_convert_to_float:\n",
        "        row[column] = float(row[column])\n",
        "\n",
        "columns_to_convert_to_int = [0, 1, 5, 6, 8, 9, 10, 11]\n",
        "\n",
        "# Convert columns to int\n",
        "for row in data:\n",
        "    for column in columns_to_convert_to_int:\n",
        "        try:\n",
        "            # Attempt to convert the value to an integer\n",
        "            row[column] = int(row[column])\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "\n",
        "# Remove rows where first column contains ip address\n",
        "rows_to_remove = []\n",
        "for index, row in enumerate(data):\n",
        "    # If the first column is an IP address\n",
        "    if row[0].count('.') == 3:\n",
        "        rows_to_remove.append(index)\n",
        "\n",
        "# Delete the rows by index\n",
        "for index in sorted(rows_to_remove, reverse=True):\n",
        "    del data[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY1cMYur4kci",
        "outputId": "0d0dfb4b-12b8-4f14-b133-b00e3dc2a629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before one-hot encoding features:\n",
            "[['56305' '23' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0' '1']\n",
            " ['41101' '23' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0' '1']\n",
            " ['60905' '23' 'tcp' 'Unkown' '2.998796' '0' '0' 'S0' 'S' '3' '180' '0'\n",
            "  '0' '1']\n",
            " ['44301' '23' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0' '1']\n",
            " ['50244' '23' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0' '1']\n",
            " ['34243' '49560' 'tcp' 'Unkown' '2.998804' '0' '0' 'S0' 'S' '3' '180'\n",
            "  '0' '0' '0']\n",
            " ['34840' '21288' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0'\n",
            "  '0']\n",
            " ['58525' '23' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0' '1']\n",
            " ['43849' '8080' 'tcp' 'Unkown' '0.0' '0' '0' 'S0' 'S' '1' '60' '0' '0'\n",
            "  '1']]\n",
            "(14,)\n",
            "After one-hot encoding features:\n",
            "[['56305' '23' '0.0' ... '0' '0' '1']\n",
            " ['41101' '23' '0.0' ... '0' '0' '1']\n",
            " ['60905' '23' '0.0' ... '0' '0' '1']\n",
            " ...\n",
            " ['34840' '21288' '0.0' ... '0' '0' '0']\n",
            " ['58525' '23' '0.0' ... '0' '0' '1']\n",
            " ['43849' '8080' '0.0' ... '0' '0' '1']]\n",
            "(160,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# columns_to_onehot = [0, 1, 2, 3, 7, 8]\n",
        "columns_to_onehot = [2, 3, 7, 8]\n",
        "\n",
        "print('Before one-hot encoding features:')\n",
        "print(data[1:10])\n",
        "print(data[1].shape)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse_output=True)\n",
        "\n",
        "dataCopy = data.copy()\n",
        "\n",
        "addedCols = 0\n",
        "for column in columns_to_onehot:\n",
        "    column_values = data[:, column]\n",
        "    onehot_encoded = onehot_encoder.fit_transform(column_values.reshape(-1, 1)).toarray()\n",
        "    dataCopy = np.delete(dataCopy, column + addedCols, axis=1)\n",
        "\n",
        "    # Insert the new columns\n",
        "    for i, encoded_column in enumerate(onehot_encoded.T):\n",
        "        dataCopy = np.insert(dataCopy, column + i + addedCols, encoded_column, axis=1)\n",
        "\n",
        "    addedCols += onehot_encoded.shape[1] - 1\n",
        "\n",
        "data = dataCopy\n",
        "\n",
        "\n",
        "print('After one-hot encoding features:')\n",
        "print(data[1:10])\n",
        "print(data[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iqgFaMLi4rvY"
      },
      "outputs": [],
      "source": [
        "# Check if any of the data contains strings\n",
        "for row in data:\n",
        "    for column in row:\n",
        "        if isinstance(column, str):\n",
        "            #Convert the value to a float, if possible\n",
        "            try:\n",
        "                column = float(column)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "# Initialize an empty list to store preprocessed data\n",
        "preprocessed_data = []\n",
        "\n",
        "# Iterate through the rows in the data\n",
        "for row in data:\n",
        "    try:\n",
        "        # Convert all columns to floats in this row\n",
        "        float_row = [float(column) if column != '-' else 0.0 for column in row]\n",
        "        preprocessed_data.append(float_row)\n",
        "    except ValueError:\n",
        "        print('Skipping row with non-convertible values:', row)\n",
        "\n",
        "data = preprocessed_data\n",
        "\n",
        "# Check if data contains strings\n",
        "for row in data:\n",
        "    for column in row:\n",
        "        if isinstance(column, str):\n",
        "            print('Error: String found in data: ', column)\n",
        "            break\n",
        "\n",
        "preprocessed_data = data\n",
        "\n",
        "# Convert preprocessed_data to a normal Python list of lists\n",
        "preprocessed_data = [list(row) for row in preprocessed_data]\n",
        "\n",
        "# # Print the preprocessed data\n",
        "# for row in preprocessed_data:\n",
        "#     print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tUccZuLH9_fE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Separate data\n",
        "\n",
        "# Define the split ratios for training, validation, and test datasets\n",
        "train_ratio = 0.70  # 70% for training\n",
        "val_ratio = 0.15   # 15% for validation\n",
        "test_ratio = 0.15  # 15% for testing\n",
        "\n",
        "train_val_indices = int((train_ratio + val_ratio) * len(preprocessed_data))\n",
        "\n",
        "train_val_data = preprocessed_data[:train_val_indices]\n",
        "test_data = preprocessed_data[train_val_indices:]\n",
        "\n",
        "# Shuffle the data randomly\n",
        "random.shuffle(train_val_data)\n",
        "random.shuffle(test_data)\n",
        "\n",
        "# Calculate the split points\n",
        "total_records = len(train_val_data)\n",
        "train_split = int(train_ratio * total_records)\n",
        "val_split = int(val_ratio * total_records)\n",
        "\n",
        "# Split the data into training, validation\n",
        "train_data = train_val_data[:train_split]\n",
        "val_data = train_val_data[train_split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jcON9qncg5S",
        "outputId": "9065e3d4-de88-47d7-dfe6-31dccd9101e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[55322.0, 8080.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 60.0, 0.0, 0.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "print((train_val_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kq8Y8Zwa-FU8"
      },
      "outputs": [],
      "source": [
        "train_labels = []\n",
        "val_labels = []\n",
        "test_labels = []\n",
        "\n",
        "# Separate the labels from features vectors\n",
        "train_temp = []\n",
        "for row in train_data:\n",
        "    newRow = []\n",
        "    newRow = row[:len(row)-1]\n",
        "    train_temp.append(newRow)\n",
        "    train_labels.append(row[-1])\n",
        "\n",
        "val_temp = []\n",
        "for row in val_data:\n",
        "    newRow = []\n",
        "    newRow = row[:len(row)-1]\n",
        "    val_temp.append(newRow)\n",
        "    val_labels.append(row[-1])\n",
        "\n",
        "test_temp = []\n",
        "for row in test_data:\n",
        "    newRow = []\n",
        "    newRow = row[:len(row)-1]\n",
        "    test_temp.append(newRow)\n",
        "    test_labels.append(row[-1])\n",
        "\n",
        "train_data = train_temp\n",
        "val_data = val_temp\n",
        "test_data = test_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDem_hw0-Hd_",
        "outputId": "a687cecc-14c3-4377-d227-0a6cd0973147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(602114, 159)\n",
            "(602114,)\n",
            "(258049, 159)\n",
            "(258049,)\n"
          ]
        }
      ],
      "source": [
        "# Convert your data to NumPy arrays\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Print rows and columns of the data\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(val_data.shape)\n",
        "print(val_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCjP1WM6_z_A",
        "outputId": "8c78509c-db98-4e72-9983-d53390526410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-tuner'...\n",
            "remote: Enumerating objects: 9259, done.\u001b[K\n",
            "remote: Counting objects: 100% (766/766), done.\u001b[K\n",
            "remote: Compressing objects: 100% (306/306), done.\u001b[K\n",
            "remote: Total 9259 (delta 580), reused 550 (delta 460), pack-reused 8493\u001b[K\n",
            "Receiving objects: 100% (9259/9259), 2.18 MiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (6600/6600), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/keras-team/keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acmy5CjtA76R",
        "outputId": "accbf8d6-a97b-4229-8bd8-d4869e1345b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/keras-tuner\n"
          ]
        }
      ],
      "source": [
        "cd keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHdcDLJlA9zE",
        "outputId": "5fea4456-7f7e-4566-a893-4a62ae44d24c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/keras-tuner\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner==1.4.7) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner==1.4.7) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner==1.4.7) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner==1.4.7)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner==1.4.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner==1.4.7) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner==1.4.7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner==1.4.7) (2024.2.2)\n",
            "Building wheels for collected packages: keras-tuner\n",
            "  Building wheel for keras-tuner (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.4.7-py3-none-any.whl size=183062 sha256=d2be1362b70f3887ff7074b435a8d1fedd4d350b150d946828e01ae0ee565a02\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/f2/f6/4d216b7ba0b7c0374eb8c129c16da679bd15329b761cbad121\n",
            "Successfully built keras-tuner\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYQBR32XBFT1",
        "outputId": "1642d51c-7640-4446-fbc3-46bc8d12bfcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-f829eee6c585>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner\n"
          ]
        }
      ],
      "source": [
        "import kerastuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jwKEhOQZ-JRZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from kerastuner import Tuner\n",
        "from kerastuner.tuners import BayesianOptimization\n",
        "\n",
        "# Check if train data contains strings\n",
        "for row in train_data:\n",
        "    for column in row:\n",
        "        if isinstance(column, str):\n",
        "            print('Error: String found in data: ', column)\n",
        "            break\n",
        "\n",
        "# Check if val data contains strings\n",
        "for row in val_data:\n",
        "    for column in row:\n",
        "        if isinstance(column, str):\n",
        "            print('Error: String found in data: ', column)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsfKAImG-K1n",
        "outputId": "dfc2a4e8-466b-4662-f170-df285070f100"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geYCsald-MZE",
        "outputId": "364dcb88-db9c-46ba-c21c-855e48a94a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define your XGBoost classifier and hyperparameter search space\n",
        "xgb_model = XGBClassifier()\n",
        "param_space = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "}\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_model,\n",
        "    param_space,\n",
        "    n_iter=5,  # Adjust the number of iterations as needed\n",
        "    scoring='accuracy',  # Use the appropriate scoring metric\n",
        "    n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
        "    cv=5,  # Number of cross-validation folds\n",
        "    random_state=42,  # Set a random seed for reproducibility\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "# Perform hyperparameter optimization\n",
        "random_search.fit(train_data, train_labels)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_xgb_hps = random_search.best_params_\n",
        "best_xgb_model = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVYzN-R6-PZG",
        "outputId": "0dd07a09-a51d-4bdd-b18d-84b6fc4ea78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'subsample': 1.0, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n"
          ]
        }
      ],
      "source": [
        "print(best_xgb_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HuSn9ZT-RJU",
        "outputId": "f8ed8c7d-d86a-4d3c-eccd-6a82a243d4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy (Random Forest): 0.9994380912152343\n",
            "Validation Classification Report (XGBoost):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00    119241\n",
            "         1.0       1.00      1.00      1.00    138808\n",
            "\n",
            "    accuracy                           1.00    258049\n",
            "   macro avg       1.00      1.00      1.00    258049\n",
            "weighted avg       1.00      1.00      1.00    258049\n",
            "\n",
            "Validation Confusion Matrix (XGBoost):\n",
            "[[119106    135]\n",
            " [    10 138798]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Evaluate the Random Forest model on the validation data\n",
        "sgboost_val_predictions = random_search.predict(val_data)\n",
        "sgboost_val_accuracy = np.mean(sgboost_val_predictions == val_labels)\n",
        "print(\"Validation Accuracy (Random Forest):\", sgboost_val_accuracy)\n",
        "\n",
        "# Calculate and print classification report and confusion matrix for Random Forest\n",
        "sgboost_val_report = classification_report(val_labels, sgboost_val_predictions)\n",
        "sgboost_val_confusion = confusion_matrix(val_labels, sgboost_val_predictions)\n",
        "print(\"Validation Classification Report (XGBoost):\")\n",
        "print(sgboost_val_report)\n",
        "print(\"Validation Confusion Matrix (XGBoost):\")\n",
        "print(sgboost_val_confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3x-LoLl-Vk1",
        "outputId": "d6b97b73-ea54-4393-ad09-9e33d56aea83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy (SGBoost): 0.9993939154380278\n",
            "Test Classification Report (XGBoost):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     74101\n",
            "         1.0       1.00      1.00      1.00     77693\n",
            "\n",
            "    accuracy                           1.00    151794\n",
            "   macro avg       1.00      1.00      1.00    151794\n",
            "weighted avg       1.00      1.00      1.00    151794\n",
            "\n",
            "Test Confusion Matrix (XGBoost):\n",
            "[[74032    69]\n",
            " [   23 77670]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Random Forest model on the test data\n",
        "sgboost_test_predictions = random_search.predict(test_data)\n",
        "sgboost_test_accuracy = np.mean(sgboost_test_predictions == test_labels)\n",
        "print(\"Test Accuracy (SGBoost):\", sgboost_test_accuracy)\n",
        "\n",
        "# Calculate and print classification report and confusion matrix for Random Forest\n",
        "sgboost_test_report = classification_report(test_labels, sgboost_test_predictions)\n",
        "sgboost_test_confusion = confusion_matrix(test_labels, sgboost_test_predictions)\n",
        "print(\"Test Classification Report (XGBoost):\")\n",
        "print(sgboost_test_report)\n",
        "print(\"Test Confusion Matrix (XGBoost):\")\n",
        "print(sgboost_test_confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilsEIgYUawhN",
        "outputId": "36526873-5f7e-4d4e-e4db-708e224d148d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 ... 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# Save the trained model to a file\n",
        "dump(best_xgb_model, 'xgboost_model.joblib')\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = load('xgboost_model.joblib')\n",
        "\n",
        "# Now you can use the loaded model to make predictions\n",
        "predictions = loaded_model.predict(test_data)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f9FPghkKL-O1"
      },
      "outputs": [],
      "source": [
        "best_xgb_model.save_model('/my_model.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spWXj51DvQqh",
        "outputId": "5813eea2-792b-4e1b-ea8e-98002a11662d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scapy\n",
            "  Downloading scapy-2.5.0.tar.gz (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scapy\n",
            "  Building wheel for scapy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scapy: filename=scapy-2.5.0-py2.py3-none-any.whl size=1444327 sha256=7ece5b22754799551e80f51f614ed72fe2890a229e9fb96ab77cfc626a264bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/b7/03/8344d8cf6695624746311bc0d389e9d05535ca83c35f90241d\n",
            "Successfully built scapy\n",
            "Installing collected packages: scapy\n",
            "Successfully installed scapy-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2T7DRqFvS3l",
        "outputId": "2d95934a-354c-4de8-84dd-678741f656c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoc_8rWvsa-Z"
      },
      "outputs": [],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saZZp5YisJye"
      },
      "outputs": [],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWAyLMA4L_a5"
      },
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "perstest = data\n",
        "\n",
        "# Save the trained model to a file\n",
        "dump(best_xgb_model, 'xgboost_model.joblib')\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = load('xgboost_model.joblib')\n",
        "\n",
        "# Now you can use the loaded model to make predictions\n",
        "predictions = loaded_model.predict(perstest)\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
